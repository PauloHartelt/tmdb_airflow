# Projeto ETL TMDB â€“ Data Warehouse com Airflow

## ğŸ“Œ DescriÃ§Ã£o

Este projeto implementa um pipeline completo de **ETL (Extract, Transform, Load)** utilizando:

- **Apache Airflow** (via Docker) como orquestrador;
- **MySQL** como banco de dados analÃ­tico (Data Warehouse);
- **Docker Compose** para orquestraÃ§Ã£o dos serviÃ§os;
- **Python + Pandas** para tratamento dos dados;
- Arquitetura **Bronze â†’ Silver â†’ Gold** inspirada em Data Lakehouse.

O objetivo Ã© processar dados do **The Movie Database (TMDB)**, armazenados inicialmente em arquivo CSV, transformar esses dados em tabelas dimensionais e fato e carregÃ¡-los em um **Data Warehouse relacional**.

---

## ğŸ”„ Pipeline ETL

### ğŸ”¹ **1. ExtraÃ§Ã£o (Bronze Layer)**  
- LÃª o arquivo CSV original (`tmdb_raw.csv`).
- Realiza limpeza leve (datas, tipos numÃ©ricos, booleanos).
- Salva um arquivo Bronze padronizado (`tmdb_bronze.csv`).

### ğŸ”¹ **2. TransformaÃ§Ã£o (Silver Layer)**  
- Enriquecimento dos dados.
- GeraÃ§Ã£o de dimensÃµes:
  - `Dim_Filme`, `Dim_Tempo`, `Dim_Idioma`, `Dim_Genero`, `Dim_Companhia`, `Dim_Pais`, `Dim_Keyword`
- GeraÃ§Ã£o das tabelas fato e associativas:
  - `Fato_Filme`, `Filme_Genero`, `Filme_Companhia`, `Filme_Pais`, `Filme_Keyword`

### ğŸ”¹ **3. Carga (Gold Layer / DWH)**  
- InserÃ§Ã£o em banco MySQL via SQLAlchemy.
- DimensÃµes persistentes.
- Fatos com append incremental.

<img width="1060" height="691" alt="Diagrama do projeto drawio" src="https://github.com/user-attachments/assets/edd6be91-f8cc-4ffc-9a65-95b8ef717919" />

---

## ğŸ› ï¸ ServiÃ§os Docker Utilizados

- Airflow Webserver â€” porta 8080  
- Airflow Scheduler  
- Airflow DB  
- PostgreSQL DWH  
- Volumes persistentes de logs e banco  

Definidos em `docker-compose.yml`.

---

## â–¶ï¸ Como executar o projeto

### **1. Extraia o arquivo TMDB_movie_dataset_v11 na pasta data_sources para a mesma pasta**

### **2. Instalar dependÃªncias**
```bash
pip install -r requirements.txt
```

### **3. Iniciar Docker Desktop**

### **4. Subir containers**
```bash
docker compose up
```

### **5. Acessar o Airflow**
```
http://localhost:8080/login/
```
Login:
- admin / admin

### **5. Executar a DAG**
Menu â†’ tmdb_etl_pipeline_fast â†’ Trigger DAG

---

## ğŸ—‚ Estrutura do Projeto

```bash
tmdb-etl-project/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ tmdb_dag.py
â”‚   â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ plugins/
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ extract_helpers.py
â”‚       â””â”€â”€ sql_schema.sql
â”œâ”€â”€ data_sources/
â”‚   â”œâ”€â”€ TMDB_movie_dataset_v11.csv
â”‚   â””â”€â”€ outputs/
â”œâ”€â”€ airflow-db-data/...
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Projeto Eixo 4.pbix
â””â”€â”€ README.md
```
---

## ğŸ¯ Objetivos atendidos

- Pipeline real com Airflow + Docker + RDS
- Camadas Bronze/Silver/Gold  
- Modelo dimensional completo  
- Carga incremental  
- Logging estruturado
- Dados sendo consumidos no Power BI  
